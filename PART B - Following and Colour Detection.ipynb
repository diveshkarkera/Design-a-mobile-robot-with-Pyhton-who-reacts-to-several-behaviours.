{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B - Following behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import os\n",
    "\n",
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "COLORS = {\n",
    "    \"RED\": {\n",
    "        \"lower\": np.array([136, 87, 111], np.uint8),\n",
    "        \"upper\": np.array([180, 255, 255], np.uint8),\n",
    "        \"bgr\": (0, 0, 255),\n",
    "    },\n",
    "     \"GREEN\": {\n",
    "        \"lower\": np.array([35, 125, 72], np.uint8),\n",
    "        \"upper\": np.array([70, 255, 255], np.uint8),\n",
    "        \"bgr\": (0, 255, 0),\n",
    "    },\n",
    "     \"BLUE\": {\n",
    "        \"lower\": np.array([94, 80, 2], np.uint8),\n",
    "        \"upper\": np.array([120, 255, 255], np.uint8),\n",
    "        \"bgr\": (255, 0, 0),\n",
    "    }\n",
    "}\n",
    "\n",
    "OBSTACLE_MAX_DEPTH = 500\n",
    "\n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "width_left_edge = 100\n",
    "width_right_edge = 200\n",
    "\n",
    "MAX_DEPTH = 500\n",
    "\n",
    "robot_state = \"FOLLOWING\"\n",
    "\n",
    "\n",
    "MIN_COLOUR_AREA = 5000\n",
    "\n",
    "color_to_detect = \"GREEN\"\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "# color_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    \"\"\"Converts BGR to SSD for input.\"\"\"\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \"\"\"Creates a model for detecting objects from the given engine.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camera(SingletonConfigurable):\n",
    "    \"\"\"Captures real time camera information from Intel realsense sensor D435i.\"\"\"\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "        \n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            \n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            \n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "            \n",
    "            self.depth_image = depth_image             \n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "    \n",
    "    def right_collision(self):\n",
    "        \"\"\"Returns the min depth for all objects on right side of the frame.\"\"\"\n",
    "        \n",
    "        right_image = np.copy(self.depth_image)\n",
    "        \n",
    "        right_image[:160,:]=0         # TOP   \n",
    "        right_image[290:480, :]=0     # BOTTOM\n",
    "        right_image[160:290, :480]=0  # LEFT\n",
    "        \n",
    "        # Clear noise from central area\n",
    "        right_image[right_image<100]=0\n",
    "        right_image[right_image>1000]=0\n",
    "        \n",
    "        # Set one of the values to be a non-zero to avoid error\n",
    "        right_image[0,0]=2000\n",
    "        \n",
    "        return right_image[right_image!=0].min()\n",
    "    \n",
    "    \n",
    "    def left_collision(self):\n",
    "        \"\"\"Returns the min depth for all objects on left side the frame.\"\"\"\n",
    "\n",
    "        left_image = np.copy(self.depth_image)\n",
    "        \n",
    "        # Set rest area depth values to 0\n",
    "        left_image[:160,:]=0          # TOP  \n",
    "        left_image[290:480, :]=0      # BOTTOM\n",
    "        left_image[160:290,190:]=0    # RIGHT\n",
    "        \n",
    "        # Clear noise from central area\n",
    "        left_image[left_image<100]=0\n",
    "        left_image[left_image>1000]=0\n",
    "        \n",
    "        # Set one of the values to be a non-zero to avoid error\n",
    "        left_image[0,0]=2000        \n",
    "        \n",
    "        return left_image[left_image!=0].min()\n",
    "            \n",
    "    def best_direction(self):\n",
    "        \"\"\"Returns the direction where the depth distance is highest.\"\"\"\n",
    "        \n",
    "        left_distance = self.left_collision()\n",
    "        right_distance = self.right_collision()\n",
    "        \n",
    "        if (left_distance > right_distance):\n",
    "            return 'LEFT'\n",
    "        else:\n",
    "            return 'RIGHT'\n",
    "         \n",
    "def bgr8_to_jpeg(value):\n",
    "    \"\"\"convert numpy array to jpeg coded data for displaying.\"\"\"\n",
    "    \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_color_of_box(x,y,w,h):\n",
    "    \"\"\"Get the color area for the given coordinates.\"\"\"\n",
    "\n",
    "    imageFrame = np.copy(camera.color_value)\n",
    "    imageFrame = imageFrame[y:y+h, x:x+w]\n",
    "\n",
    "    # color space\n",
    "    hsvFrame = cv2.cvtColor(imageFrame, cv2.COLOR_BGR2HSV)\n",
    "  \n",
    "\n",
    "    # define mask        \n",
    "    mask = cv2.inRange(hsvFrame, COLORS[color_to_detect][\"lower\"], COLORS[color_to_detect][\"upper\"])\n",
    "      \n",
    "    # Morphological Transform, Dilation for each color and bitwise_and operator between\n",
    "    # imageFrame and mask determines to detect only that particular color\n",
    "    kernal = np.ones((5, 5), \"uint8\")\n",
    "    \n",
    "    mask = cv2.dilate(mask, kernal)\n",
    "    res = cv2.bitwise_and(imageFrame, imageFrame, mask = mask)\n",
    "   \n",
    "    # Creating contour to track red color\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "    areas = []\n",
    "    for pic, contour in enumerate(contours):\n",
    "        area = cv2.contourArea(contour)\n",
    "        areas.append(area)\n",
    "          \n",
    "    sum_area = sum(areas) if len(areas) else 0\n",
    "    \n",
    "    return sum_area, imageFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_depth(bbox):\n",
    "    \"\"\"Returns the min depth of the given coordinates.\"\"\"\n",
    "    \n",
    "    depth_image = np.copy(camera.depth_image)\n",
    "    \n",
    "    #we only consider the central area of the vision sensor\n",
    "    depth_image[:int(height * bbox[1]),:]=0\n",
    "    depth_image[int(height * bbox[3]):,:]=0\n",
    "    depth_image[:,:int(width * bbox[0])]=0\n",
    "    depth_image[:,int(width * bbox[2]):]=0\n",
    "            \n",
    "    #For object avoidance, we don't consider the distance that are lower than 100mm or bigger than 1000mm\n",
    "    depth_image[depth_image<100]=0\n",
    "    depth_image[depth_image>1000]=0\n",
    "            \n",
    "    #If all of the values in the depth image is 0, the depth[depth!=0] command will fail\n",
    "    #we set a specific value here to prevent this failure\n",
    "    depth_image[0,0]=2000\n",
    "                    \n",
    "    return(depth_image[depth_image!=0].min())\n",
    "\n",
    "# source 1 - [s1]\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    \n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "# end of source 1 - [s1]\n",
    "\n",
    "def detect_section(coord):\n",
    "    \"\"\"Detects what section the object is in and calculates the turn gain.\n",
    "    turn gain is between 0 and 1.\"\"\"\n",
    "    \n",
    "    direction = \"CENTER\"\n",
    "    center_x = coord[0] \n",
    "    x = center_x * width\n",
    "    \n",
    "    if(x < width_left_edge):\n",
    "        direction= \"LEFT\"\n",
    " \n",
    "    elif(x > width_right_edge):\n",
    "        direction= \"RIGHT\"\n",
    "    return direction\n",
    "\n",
    "def move_robot(center):\n",
    "    \"\"\" Move robot based on the center.\"\"\"\n",
    "    \n",
    "    speed = 0.5\n",
    "    left_speed = float(speed + 0.5 * center[0])\n",
    "    right_speed = float(speed - 0.5 * center[0])        \n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)\n",
    "\n",
    "def turn_left():\n",
    "    \"\"\"Turn robot left on the spot.\"\"\"\n",
    "    \n",
    "    speed = 0.1\n",
    "    left_speed = float(speed - 0.5 )\n",
    "    right_speed = float(speed + 0.5 )  \n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)\n",
    "\n",
    "\n",
    "def turn_right():\n",
    "    \"\"\"Turn robot right on the spot.\"\"\"\n",
    "    speed = 0.1\n",
    "    left_speed = float(speed + 0.5 )\n",
    "    right_speed = float(speed - 0.5 )   \n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)\n",
    "    \n",
    "    \n",
    "def turn_forward_right():\n",
    "    \"\"\"Turn robot forward right on the spot.\"\"\"\n",
    "    \n",
    "    speed = 0.5\n",
    "    right_speed = float(speed / 5)\n",
    "    left_speed = float(speed*2 )  \n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)\n",
    "    \n",
    "def turn_forward_left():\n",
    "    \"\"\"Turn robot forward left on the spot.\"\"\"\n",
    "    \n",
    "    speed = 0.5\n",
    "    left_speed = float(speed / 5)\n",
    "    right_speed = float(speed *2)  \n",
    "    robot.set_motors(left_speed, right_speed, left_speed, right_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Behavior(object):\n",
    "    \"\"\"Behaviour class for robot.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.target_center = None\n",
    "        self.target_direction = None        \n",
    "        self.target_depth = None\n",
    "        self.target_last_position = None\n",
    "        pass\n",
    "    \n",
    "    def move(self):\n",
    "        pass\n",
    "    \n",
    "class Follow(Behavior):\n",
    "    \"\"\"\n",
    "        Implements the follow behaviour to find, follow, and stop the robot when near the target.        \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def find_target(self, _robot_state, use_robot_state=False): \n",
    "        \"\"\"Turn the robot towards the target using target_last_position.\"\"\"\n",
    "        if self.target_last_position is not None:\n",
    "            if use_robot_state and _robot_state == \"AVOIDING\":\n",
    "                return\n",
    "            \n",
    "            new_turn_direction = detect_section(self.target_last_position)\n",
    "            if new_turn_direction == \"LEFT\": \n",
    "                turn_left()\n",
    "            else:\n",
    "                turn_right()\n",
    "        \n",
    "        else:\n",
    "            robot.stop()\n",
    "            self.target_detected = False\n",
    "        \n",
    "    def target_reached(self):\n",
    "        \"\"\"Stop the robot when when in target range.\"\"\"\n",
    "        self.target_last_position = None\n",
    "        robot.stop()\n",
    "    \n",
    "    def move(self, center):\n",
    "        \"\"\"Move towards the target center when its detected.\"\"\"\n",
    "        self.target_center = center\n",
    "        move_robot(center)\n",
    "               \n",
    "        \n",
    "    \n",
    "class Avoid(Behavior):\n",
    "    \"\"\"Implements the avoid behaviour to move away from .\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def move(self, last_follow_direction):\n",
    "        \"\"\"Move away from obstacle based on the target's last_follow_direction and obstacle direction.\"\"\"\n",
    "        if self.target_direction == 'CENTER':\n",
    "            \n",
    "            if last_follow_direction == 'LEFT':\n",
    "                turn_forward_left()\n",
    "            \n",
    "            elif last_follow_direction == 'RIGHT':\n",
    "                turn_forward_right()\n",
    "            \n",
    "            else:                \n",
    "                best_direction = camera.best_direction()\n",
    "                if(best_direction == 'LEFT'):\n",
    "                    turn_left()\n",
    "                else:\n",
    "                    turn_right()\n",
    "\n",
    "        elif self.target_direction == 'RIGHT':\n",
    "            turn_forward_left()\n",
    "\n",
    "        else:\n",
    "            turn_forward_right()\n",
    "    \n",
    "    \n",
    "follow_behaviour = Follow()\n",
    "avoid_behaviour = Avoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_detections(detections, detect_color = False, image = None, draw_rect=False, draw_color=(255, 0, 0) ):    \n",
    "    \"\"\"Returns the depths and colors for each detections.\"\"\"\n",
    "    \n",
    "    depths = np.array([])        \n",
    "    colors = np.array([])\n",
    "    \n",
    "    for det in detections:    \n",
    "        \n",
    "        bbox = det['bbox']\n",
    "        depths = np.append(depths, min_depth(bbox)) \n",
    "\n",
    "        if draw_rect :\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), draw_color , 2)\n",
    "        \n",
    "        if detect_color:\n",
    "            x, y = int(width * bbox[0]), int(height * bbox[1])\n",
    "            w = int(width * bbox[2]) - int(width * bbox[0])\n",
    "            h = int(height * bbox[3]) - int(height * bbox[1])\n",
    "\n",
    "            target_area, imageFrame = detect_color_of_box(x, y, w, h)\n",
    "            colors = np.append(colors, target_area)\n",
    "        \n",
    "    return depths, colors\n",
    "\n",
    "def processing(change):\n",
    "    \"\"\"Runs every frame based on camera feed and processes the image.\"\"\"\n",
    "    \n",
    "    global robot_state\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    obstacle = None\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    human_detections = [d for d in detections[0] if d['label'] == 1]\n",
    "    \n",
    "    object_detections = [d for d in detections[0] if d['label'] != 1]\n",
    "    \n",
    "    # get depth and colors for each detections.\n",
    "    detection_depths, detection_colors =  process_detections(human_detections, detect_color = True)    \n",
    "    object_detection_depths, _ =  process_detections(object_detections, image=image, draw_rect=True)\n",
    "    \n",
    "    # Get the closest object and show on screen.\n",
    "    if len(object_detections) != 0:        \n",
    "        closest_idx = np.where(object_detection_depths == np.amin(object_detection_depths))[0][0]\n",
    "        obstacle = object_detections[closest_idx]\n",
    "        obstacle_depth = object_detection_depths[closest_idx] \n",
    "        bbox = obstacle['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 0, 255), 2)\n",
    "           \n",
    "    if len(human_detections)==0 :\n",
    "        follow_behaviour.find_target(robot_state)\n",
    "        robot_state = \"SEARCHING\"\n",
    "    else:\n",
    "        # get the index of max colour area of detected humans\n",
    "        idx = np.where(detection_colors == np.amax(detection_colors))[0][0]\n",
    "                \n",
    "        if detection_colors[idx] < MIN_COLOUR_AREA:\n",
    "            follow_behaviour.find_target(robot_state, use_robot_state=True)\n",
    "            robot_state = \"SEARCHING\"\n",
    "        else:\n",
    "            target = human_detections[idx]\n",
    "            bbox = target['bbox']\n",
    "            target_depth = detection_depths[idx]\n",
    "            # Draw rectangle for target\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 2)\n",
    "            \n",
    "            # Obstacle collision avoidance\n",
    "            if obstacle is not None and obstacle_depth <= OBSTACLE_MAX_DEPTH and obstacle_depth < target_depth:\n",
    "                                \n",
    "                robot_state = \"AVOIDING\" \n",
    "\n",
    "                target_center = detection_center(target)                    \n",
    "                target_direction = detect_section(target_center)\n",
    "\n",
    "                avoid_behaviour.target_center = detection_center(obstacle) \n",
    "                avoid_behaviour.target_direction = detect_section(avoid_behaviour.target_center)                    \n",
    "                avoid_behaviour.move(target_direction)\n",
    "\n",
    "                follow_behaviour.target_last_position = target_center                    \n",
    "            \n",
    "            # Robot reached target\n",
    "            elif(target_depth <= MAX_DEPTH):                \n",
    "                follow_behaviour.target_reached()\n",
    "                robot_state = \"STOPPED\"\n",
    "\n",
    "            # Follow the target\n",
    "            else:\n",
    "                robot_state = \"FOLLOWING\"\n",
    "                center = detection_center(target)\n",
    "                follow_behaviour.move(center)\n",
    "                follow_behaviour.target_last_position = center\n",
    "#                 print(center)\n",
    "    \n",
    "    cv2.putText(image, robot_state, (320,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv2.LINE_AA)            \n",
    "    \n",
    "   \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "        \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d35cbdf3fa42868066bb0eb0f138b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.VBox([\n",
    "    image_widget,\n",
    "    label_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.stop()\n",
    "camera.start()\n",
    "time.sleep(1.0)\n",
    "robot.stop()\n",
    "follow_behaviour.target_last_position = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[s1]- https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/notebooks/object_following/live_demo.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
